# CLAUDE.md - Deep Linear Networks Research Project

üöÄ Project Vision
Build and scale deep linear networks to challenge the nonlinearity assumption in deep learning, starting with time series applications.

üìã Executive Summary
We're exploring a radical hypothesis: deep linear networks (1000+ layers, no activation functions) might outperform traditional nonlinear networks for certain domains, particularly time series. This is based on recent theoretical breakthroughs showing that depth alone creates rich dynamics through Riemannian geometry, even without nonlinearity.

Why This Matters
Nobody has tried it at scale - massive research gap
Theory suggests it should work - Riemannian gradient flows, infinite depth limits
Practical advantages - 10-100x faster, perfectly interpretable, guaranteed convergence
Time series is the perfect test - naturally linear dynamics, superposition principle
What We're Building
A comprehensive framework for deep linear networks on time series, including:

Core architectures (100-1000+ layers)
Efficient implementations using structured matrices
Benchmarks against transformers/LSTMs
Theoretical analysis tools
Interpretability visualizations
üßÆ Theoretical Foundation
Key Insight: The Geometry of Deep Linear Networks
From Menon et al. (2023-2024), we know that deep linear networks with N layers:

python

# Network structure

W = W_N √ó W_{N-1} √ó ... √ó W_1  # End-to-end is just matrix multiplication

# BUT the dynamics are Riemannian gradient flow

dW/dt = -grad_{g_N} E(W)  # g_N is depth-dependent metric

# The metric has explicit form

g_N(Z,Z) = Tr(Z^T A_{N,W}^{-1} Z)

# where A_{N,W} encodes the depth structure

Critical Discoveries
Invariant Manifolds: Training dynamics confined to balanced varieties
Implicit Regularization: Entropy S(W) = log vol(O_W) provides selection
Infinite Depth Limit: Converges to continuous flows
Phase Transitions: Rank drops suddenly during training
Why Linear Might Be Enough
Composition Power: Infinite linear layers can approximate nonlinear functions
Optimization Dynamics: The training trajectory is highly nonlinear even if function isn't
Structured Linear Models Work: S4/Mamba (90% linear) match GPT-3 performance
üèóÔ∏è Architecture Design
Core Architecture: DeepLinearTimeSeries

Main architecture for deep linear time series modeling.
Key innovations:

1. Balanced initialization (critical for deep networks)
2. Temporal mixing layers (linear attention without softmax)
3. Residual connections (maintain gradient flow)
4. Multi-scale processing
